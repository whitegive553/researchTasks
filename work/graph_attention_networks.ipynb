{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Networks"
      ],
      "metadata": {
        "id": "YKAm9ese0A7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## technical terms\n",
        "\n",
        "\n",
        "### grid-like structure:\n",
        "semantic segmentation: classification each unit in the picture\n",
        "machine translation: using attention weight. source language sequence and target language sequence weight matrix.\n",
        "eg. text and picture\n",
        "\n",
        "### graph domain convolutions:\n",
        "spectral and non-spectral approaches.    \n",
        "spectral: Laplacian get eigonvalue and eigonvector. O(N^3) && hard to transfer.    \n",
        "non-spectral: only focusing on neighbours, different way of aggregation and activation.\n",
        "\n",
        "### attention machanisms:\n",
        "sequence? self-attention. dig out the key part in a long sequence.\n",
        "\n",
        "### masked attention:\n",
        "by matrix tranformation, can only see the sentence before the word. implemented by using matrix.\n",
        "\n",
        "### transductive learning:\n",
        "can access the test set, but cant access the label. perform good at the already Known dataset.\n",
        "\n",
        "### inductive learning:\n",
        "hyperspace of Supervised learning. can not access to test set. Better to extend to new dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "255tCEn0BgYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## structure\n",
        "\n",
        "CNN - perform good at grid data structure    \n",
        "|    \n",
        "extend to graph data structure - could deal with arbitrary data structure    \n",
        "|    \n",
        "bring in convolution - spectral and non-spectral approach    \n",
        "|    \n",
        "non-spectral - attention structure     \n",
        "|    \n",
        "parallal && easy to extend && doesnt depend on number of neighbours.    "
      ],
      "metadata": {
        "id": "2Ul2tUrcFpfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAT\n",
        "\n",
        "### $$e_{ij} = a(\\mathbf{W} \\mathbf{h}_i, \\mathbf{W} \\mathbf{h}_j)$$\n",
        "shared linear transformation -> attention -> got factor\n",
        "### sum the factor times linear transformed input of current node's neighbour. By non-lineartransformation got hi', for inner layer, using concat, while the final layer, using average to got the overall information of current node and its neighbours."
      ],
      "metadata": {
        "id": "_gf1wOcpQIor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "focusing on node, does not depend on whole structure of graph.    \n",
        "parallal.    \n",
        "## limitation\n",
        "only work for rank2 tensor"
      ],
      "metadata": {
        "id": "JHAI6phQrJ96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question\n",
        " ## for experiment set: usual attention heads? transductive learning has fied dataset, why using more attention heads to get diverse data?\n",
        ""
      ],
      "metadata": {
        "id": "9uUkgHFeqoaY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PWXxY_aDPO0R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}